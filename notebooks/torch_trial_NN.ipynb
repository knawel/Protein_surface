{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "670991eb-37cd-44fc-9336-44c35d062fec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b793f4a-8199-48ce-b7b8-203b18e5662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "882f84df-e5bb-4b83-a781-5429d48f2df2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# local libs\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AllVertices\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AmberNN\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlogger\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Logger\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"
     ]
    }
   ],
   "source": [
    "# local libs\n",
    "from dataset import AllVertices\n",
    "from src.nn_model import AmberNN\n",
    "from src.logger import Logger\n",
    "from run_opts import config_runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f2c007-14b8-432a-8384-86f944893429",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = torch.manual_seed(142)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0975e214-7dbf-4521-9dde-0ac76f343b25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config_runtime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m n_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# number of classes\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m conf_dev \u001b[38;5;241m=\u001b[39m \u001b[43mconfig_runtime\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m train_f \u001b[38;5;241m=\u001b[39m config_runtime[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_frac\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m seed \u001b[38;5;241m=\u001b[39m config_runtime[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config_runtime' is not defined"
     ]
    }
   ],
   "source": [
    "n_cls = 3  # number of classes\n",
    "conf_dev = config_runtime['device']\n",
    "train_f = config_runtime['train_frac']\n",
    "seed = config_runtime['seed']\n",
    "learning_rate = config_runtime['learning_rate']\n",
    "batch_size = config_runtime['batch_size']\n",
    "hid_size = config_runtime['hidden_size']\n",
    "log_step = config_runtime['log_step']\n",
    "epochs = config_runtime['num_epochs']\n",
    "run_name = config_runtime['run_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "296da7a8-9ee6-4851-8472-ad7cdbca036a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters\n",
      "-------------------------\n",
      "Train fraction:    0.85\n",
      "Learning rate:     0.0001\n",
      "Batch size:        128\n",
      "Hidden layer size: 256\n",
      "Number of epochs:  12\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(\"./\", \"train\")\n",
    "\n",
    "# store parameters\n",
    "logger.print(\"Parameters\")\n",
    "logger.print(\"-------------------------\")\n",
    "logger.print(f\"Train fraction:    {train_f}\")\n",
    "logger.print(f\"Learning rate:     {learning_rate}\")\n",
    "logger.print(f\"Batch size:        {batch_size}\")\n",
    "logger.print(f\"Hidden layer size: {hid_size}\")\n",
    "logger.print(f\"Number of epochs:  {epochs}\")\n",
    "logger.print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6d3950b-b1fd-4157-8b14-4e7995215007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read list of the proteins\n",
    "proteins = []\n",
    "with open(\"../data/lists/charge_v2.txt\", 'r') as iFile:\n",
    "    for i in iFile:\n",
    "        if i[0] != '#':\n",
    "            proteins.append(i.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c8706d-3569-474b-aefb-4ef155b0dcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prots, test_prots = train_test_split(proteins, train_size=train_f, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5780c657-42fb-4de2-be3f-9cfd44482b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Data\n",
      "-------------------------\n",
      "Proteins: train 26   test 5\n"
     ]
    }
   ],
   "source": [
    "logger.print(\"\\n\")\n",
    "logger.print(\"Data\")\n",
    "logger.print(\"-------------------------\")\n",
    "logger.print(f\"Proteins: train {len(train_prots)}   test {len(test_prots)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba7f95f6-41e1-42d0-b770-ef6e7c527bee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd = AllVertices(proteins)\n",
    "# n_features = pd[0][0].shape[0]\n",
    "# train_dataset, test_dataset = random_split(pd, [train_f, 1.0 - train_f],\n",
    "#                                            generator=torch.Generator().manual_seed(seed))\n",
    "train_dataset = AllVertices(train_prots)\n",
    "test_dataset = AllVertices(test_prots)\n",
    "n_features = train_dataset[0][0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d192411-2e78-4717-be27-2fc18385c055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertices: train 279749   test 29564\n",
      "Features: 111\n"
     ]
    }
   ],
   "source": [
    "logger.print(f\"Vertices: train {len(train_dataset)}   test {len(test_dataset)}\")\n",
    "logger.print(f\"Features: {n_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb055f9-9213-45f3-bd86-22412b13ed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eacc2f6-7693-45a2-bc7c-8c1f42b5e006",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create data loaders.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mtrain_dataset\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m AmberNN(n_features, n_cls, hid_size)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "model = AmberNN(n_features, n_cls, hid_size).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d9ff3f-058f-4356-b268-604370f993fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sys.stdout.write(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e1ba0-b822-4f74-af1b-cc4e7fc41e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb82ecd-6b0f-417e-ac6d-2c6793a70077",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.stack([train_dataset[333][0], train_dataset[235][0]], axis=0)\n",
    "X = X.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba906973-125d-4415-be13-0ec5818296b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d932387-8482-4160-a007-7c4ae5927535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            \n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8adc73-27c4-4109-a595-526bd12ede90",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78818636-add2-4804-a92b-8ea564b6cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"20230301_model.pytorch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8657578-acc5-4ffe-ab61-5ad6602af999",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd8697-c197-4375-a63e-9ce8457a084a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42b336-a927-4300-bcad-19e97cb40623",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trial_pdb = \"6BOY_B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665da8ae-7150-4267-b2cf-c947f302a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ProteinRecord(Trial_pdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff67995-6dea-4c43-9e8b-1272b4296d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.f.to(device)\n",
    "p = data.p.to(device)\n",
    "y = data.y_aux.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcaba120-bf89-4cec-9919-d5e16d55dd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(X)\n",
    "    pred_probab = nn.Softmax(dim=1)(logits)\n",
    "    y_pred = pred_probab.argmax(1)\n",
    "    # print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d94e4a-aa31-40a8-8ff6-fb58ed257d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_for_paraview(f\"predicted_{Trial_pdb}.csv\", p, y_pred)\n",
    "save_for_paraview(f\"real_{Trial_pdb}.csv\", p, y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6962aca6-1e0d-4c91-9720-4557ee8c126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[580:680]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
