Parameters
-------------------------
Train fraction:    0.87
Learning rate:     0.0001
Batch size:        64
Hidden layer size: 256
Number of epochs:  80
-------------------------


Data
-------------------------
Proteins: train 38   test 6
Proteins in test set: 4wfo_A 7MX9_A 3sov_A 2EG5_E 1zt3_A 1FZW_B
Vertices: train 457010   test 82546
Features: 106
Epoch 1
-------------------------------
loss: 1.117945  [   64/457010]
loss: 1.104345  [32832/457010]
loss: 1.028573  [65600/457010]
loss: 1.048707  [98368/457010]
loss: 1.139429  [131136/457010]
loss: 1.147185  [163904/457010]
loss: 1.089580  [196672/457010]
loss: 1.103181  [229440/457010]
loss: 1.020588  [262208/457010]
loss: 1.144863  [294976/457010]
loss: 1.065278  [327744/457010]
loss: 0.983277  [360512/457010]
loss: 1.031201  [393280/457010]
loss: 1.054119  [426048/457010]
Test Error: 
 Accuracy: 58.3%, Avg loss: 1.038352 
Epoch 2
-------------------------------
loss: 1.182915  [   64/457010]
loss: 1.124582  [32832/457010]
loss: 0.918119  [65600/457010]
loss: 0.987269  [98368/457010]
loss: 1.202428  [131136/457010]
loss: 1.215158  [163904/457010]
loss: 1.084943  [196672/457010]
loss: 1.114851  [229440/457010]
loss: 0.955007  [262208/457010]
loss: 1.194705  [294976/457010]
loss: 1.045468  [327744/457010]
loss: 0.905249  [360512/457010]
loss: 0.992233  [393280/457010]
loss: 1.034414  [426048/457010]
Test Error: 
 Accuracy: 58.3%, Avg loss: 1.011173 
Epoch 3
-------------------------------
loss: 1.242422  [   64/457010]
loss: 1.147714  [32832/457010]
loss: 0.837046  [65600/457010]
loss: 0.944166  [98368/457010]
loss: 1.259064  [131136/457010]
loss: 1.276120  [163904/457010]
loss: 1.089606  [196672/457010]
loss: 1.129639  [229440/457010]
loss: 0.907405  [262208/457010]
loss: 1.238935  [294976/457010]
loss: 1.035318  [327744/457010]
loss: 0.848685  [360512/457010]
loss: 0.966080  [393280/457010]
loss: 1.023468  [426048/457010]
Test Error: 
 Accuracy: 58.3%, Avg loss: 0.994026 
Epoch 4
-------------------------------
loss: 1.295794  [   64/457010]
loss: 1.170477  [32832/457010]
loss: 0.775427  [65600/457010]
loss: 0.914323  [98368/457010]
loss: 1.309171  [131136/457010]
loss: 1.329936  [163904/457010]
loss: 1.097062  [196672/457010]
loss: 1.145929  [229440/457010]
loss: 0.872668  [262208/457010]
loss: 1.277610  [294976/457010]
loss: 1.031232  [327744/457010]
loss: 0.805773  [360512/457010]
loss: 0.947581  [393280/457010]
loss: 1.017784  [426048/457010]
Test Error: 
 Accuracy: 58.3%, Avg loss: 0.983368 
Epoch 5
-------------------------------
loss: 1.342526  [   64/457010]
loss: 1.193409  [32832/457010]
loss: 0.729180  [65600/457010]
loss: 0.892170  [98368/457010]
loss: 1.353189  [131136/457010]
loss: 1.375023  [163904/457010]
loss: 1.102959  [196672/457010]
loss: 1.161220  [229440/457010]
loss: 0.848009  [262208/457010]
loss: 1.312642  [294976/457010]
loss: 1.030353  [327744/457010]
loss: 0.773836  [360512/457010]
loss: 0.934969  [393280/457010]
loss: 1.015641  [426048/457010]
Test Error: 
 Accuracy: 58.3%, Avg loss: 0.976902 
Epoch 6
-------------------------------
loss: 1.381895  [   64/457010]
loss: 1.212277  [32832/457010]
loss: 0.694152  [65600/457010]
loss: 0.877071  [98368/457010]
loss: 1.389127  [131136/457010]
loss: 1.414285  [163904/457010]
loss: 1.111426  [196672/457010]
loss: 1.174912  [229440/457010]
loss: 0.829412  [262208/457010]
loss: 1.340342  [294976/457010]
loss: 1.029758  [327744/457010]
loss: 0.749318  [360512/457010]
loss: 0.927640  [393280/457010]
loss: 1.014824  [426048/457010]
Test Error: 
 Accuracy: 58.3%, Avg loss: 0.973066 
Epoch 7
-------------------------------
loss: 1.411501  [   64/457010]
loss: 1.229400  [32832/457010]
loss: 0.667174  [65600/457010]
loss: 0.867055  [98368/457010]
loss: 1.419386  [131136/457010]
loss: 1.444010  [163904/457010]
loss: 1.119307  [196672/457010]
loss: 1.187382  [229440/457010]
loss: 0.816450  [262208/457010]
loss: 1.363330  [294976/457010]
loss: 1.031312  [327744/457010]
loss: 0.731848  [360512/457010]
loss: 0.921690  [393280/457010]
loss: 1.014912  [426048/457010]
Test Error: 
 Accuracy: 58.3%, Avg loss: 0.970809 
Epoch 8
-------------------------------
loss: 1.436718  [   64/457010]
loss: 1.242714  [32832/457010]
loss: 0.647628  [65600/457010]
loss: 0.859207  [98368/457010]
loss: 1.442553  [131136/457010]
loss: 1.468849  [163904/457010]
loss: 1.126346  [196672/457010]
loss: 1.196488  [229440/457010]
loss: 0.805967  [262208/457010]
loss: 1.381305  [294976/457010]
loss: 1.032504  [327744/457010]
loss: 0.717997  [360512/457010]
loss: 0.917657  [393280/457010]
loss: 1.016248  [426048/457010]
Test Error: 
 Accuracy: 58.3%, Avg loss: 0.969527 
Epoch 9
-------------------------------
loss: 1.457148  [   64/457010]
loss: 1.252508  [32832/457010]
loss: 0.632564  [65600/457010]
loss: 0.852506  [98368/457010]
loss: 1.459512  [131136/457010]
loss: 1.487323  [163904/457010]
loss: 1.130139  [196672/457010]
loss: 1.204439  [229440/457010]
loss: 0.799055  [262208/457010]
loss: 1.397084  [294976/457010]
loss: 1.033532  [327744/457010]
loss: 0.709678  [360512/457010]
loss: 0.914562  [393280/457010]
loss: 1.017486  [426048/457010]
Test Error: 
 Accuracy: 58.3%, Avg loss: 0.968802 
Epoch 10
-------------------------------
loss: 1.469587  [   64/457010]
loss: 1.259792  [32832/457010]
loss: 0.622202  [65600/457010]
loss: 0.850328  [98368/457010]
loss: 1.475081  [131136/457010]
loss: 1.502982  [163904/457010]
loss: 1.134304  [196672/457010]
loss: 1.210004  [229440/457010]
loss: 0.793291  [262208/457010]
loss: 1.408624  [294976/457010]
loss: 1.033921  [327744/457010]
loss: 0.700694  [360512/457010]
loss: 0.913052  [393280/457010]
loss: 1.017871  [426048/457010]
Test Error: 
 Accuracy: 58.3%, Avg loss: 0.968365 
Epoch 11
-------------------------------
loss: 1.480577  [   64/457010]
loss: 1.267293  [32832/457010]
loss: 0.613395  [65600/457010]
loss: 0.847711  [98368/457010]
loss: 1.486685  [131136/457010]
loss: 1.512990  [163904/457010]
loss: 1.138048  [196672/457010]
loss: 1.215611  [229440/457010]
loss: 0.789752  [262208/457010]
loss: 1.417376  [294976/457010]
loss: 1.034251  [327744/457010]
loss: 0.695831  [360512/457010]
loss: 0.910969  [393280/457010]
loss: 1.019033  [426048/457010]
